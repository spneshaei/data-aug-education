{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"ANON\")\n",
    "\n",
    "df.head() # columns: text, label, id, test_fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find label count distribution and store it as a dict\n",
    "label_count = df['label'].value_counts().to_dict()\n",
    "label_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the maximum label count\n",
    "max_label_count = max(label_count.values())\n",
    "\n",
    "# Calculate the difference needed for each class to reach the maximum count\n",
    "label_count_diff = {label: max_label_count - count for label, count in label_count.items()}\n",
    "\n",
    "label_count_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find how many augmentations should we do per sentence, by dividing the count diff by the number of sentences already in that class\n",
    "augmentation_count_needed = {label: diff // count for label, diff, count in zip(label_count.keys(), label_count_diff.values(), label_count.values())}\n",
    "augmentation_count_needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAPPING = {\n",
    "    0: \"Description\",\n",
    "    1: \"Feelings\",\n",
    "    2: \"Evaluation\",\n",
    "    3: \"Analysis\",\n",
    "    4: \"Conclusion\",\n",
    "    5: \"Action Plan\"\n",
    "}\n",
    "\n",
    "developer_prompt = \"You are an expert in the Gibbs reflective cycle. The components of the Gibbs reflective cycle are: Description, Feelings, Evaluation, Analysis, Conclusion, and Action Plan. Description refers describing to the event or experience you are reflecting on. Feelings refers to your emotions during the event or experience. Evaluation refers to your thoughts about the event or experience, providing positive and negative aspects on what happened. Analysis refers to your understanding of the event or experience, providing reasons behind points mentioned in the Evaluation aspect. Conclusion refers to what you learned from the event or experience. Action Plan refers to what you would do differently in the future (next time). Always respond in German.\"\n",
    "user_prompts = [] # {id: XX, prompt: YY}\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    for i in range(augmentation_count_needed[row[\"label\"]]):\n",
    "        prompt = \"This sentence is a reflective sentence from the \" + MAPPING[row[\"label\"]] + \" component of the Gibbs reflective cycle:\\n\" + row[\"text\"] + \"\\n\\nFirst, think step by step on why this is a sentence of the \" + MAPPING[row[\"label\"]] + \" component of the Gibbs reflective cycle. Then, think step by step, and finally in the last line of your response (after putting a line break), please write a sentence that reflects on the same event or experience, focusing on the \" + MAPPING[row[\"label\"]] + \" component of the Gibbs reflective cycle. You should use different names, words, and terminologies in your output, but the overall meaning and content should be the same and refer to the same Gibbs component. In the *last line* of your output, just put the sentence and nothing else. Respond in German.\"\n",
    "        user_prompts.append({\"id\": row[\"id\"], \"prompt\": prompt})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(user_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "with open(\"../../../api_key.txt\", \"r\") as f:\n",
    "    api_key = f.read().strip()\n",
    "\n",
    "openai.api_key = api_key\n",
    "openai_client = OpenAI(api_key=api_key)\n",
    "\n",
    "def return_message_from_openai(messages, temperature = 1):\n",
    "    global openai_client\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=\"gpt-4o-2024-08-06\",\n",
    "        messages=messages,\n",
    "        temperature=temperature\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(user_prompts[6])\n",
    "print(\"=====\")\n",
    "print(return_message_from_openai([\n",
    "    {\n",
    "        \"role\": \"developer\",\n",
    "        \"content\": developer_prompt\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": user_prompts[6][\"prompt\"]\n",
    "    }\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "jsonl_data = []\n",
    "\n",
    "for i, prompt in enumerate(user_prompts):\n",
    "    jsonl_data.append({\n",
    "        \"custom_id\": f\"request-{i+1}-id-{prompt['id']}\",\n",
    "        \"method\": \"POST\",\n",
    "        \"url\": \"/v1/chat/completions\",\n",
    "        \"body\": {\n",
    "            \"model\": \"gpt-4o-2024-08-06\",\n",
    "            \"messages\": [\n",
    "                {\"role\": \"developer\", \"content\": developer_prompt},\n",
    "                {\"role\": \"user\", \"content\": prompt[\"prompt\"]}\n",
    "            ],\n",
    "            \"temperature\": 1.0\n",
    "        }\n",
    "    })\n",
    "\n",
    "with open(\"chain-of-thought-requests.jsonl\", \"w\") as f:\n",
    "    for entry in jsonl_data:\n",
    "        f.write(json.dumps(entry) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_input_file = openai_client.files.create(\n",
    "    file=open(\"chain-of-thought-requests.jsonl\", \"rb\"),\n",
    "    purpose=\"batch\"\n",
    ")\n",
    "\n",
    "print(batch_input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_input_file_id = batch_input_file.id\n",
    "openai_client.batches.create(\n",
    "    input_file_id=batch_input_file_id,\n",
    "    endpoint=\"/v1/chat/completions\",\n",
    "    completion_window=\"24h\",\n",
    "    metadata={\n",
    "        \"description\": \"Chain-of-thought\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FROM RETRIEVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = openai_client.batches.retrieve(\"ANON\")\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_response = openai_client.files.content(batch.output_file_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# make a new dataframe id,text-id,text,label,original_id,test_fold, by using the df we had in the beginning\n",
    "result_df = pd.DataFrame(columns=[\"id\", \"text\", \"label\", \"original_id\", \"test_fold\"])\n",
    "\n",
    "for line in file_response.text.split(\"\\n\"):\n",
    "    if line:\n",
    "        data = json.loads(line)\n",
    "        text = data[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "        text = text.split(\"\\n\")[-1]\n",
    "        \n",
    "        new_row = {\n",
    "            \"id\": data[\"custom_id\"],\n",
    "            # \"text-id\": df[df[\"id\"] == int(data[\"custom_id\"].split(\"-\")[-1])][\"text-id\"].values[0],\n",
    "            \"text\": text,\n",
    "            \"label\": df[df[\"id\"] == int(data[\"custom_id\"].split(\"-\")[-1])][\"label\"].values[0],\n",
    "            \"original_id\": int(data[\"custom_id\"].split(\"-\")[-1]),\n",
    "            \"test_fold\": df[df[\"id\"] == int(data[\"custom_id\"].split(\"-\")[-1])][\"test_fold\"].values[0]\n",
    "        }\n",
    "        result_df = pd.concat([result_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "result_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv(\"chain-of-thought-outputs.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mdev2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
