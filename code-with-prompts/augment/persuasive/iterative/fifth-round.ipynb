{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = 0\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(f\"../../../training/discourse/fourth-round/wrong-predictions-fold-{fold}.csv\")\n",
    "\n",
    "df.head() # columns: text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_count = df['label'].value_counts().to_dict()\n",
    "label_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the maximum label count\n",
    "max_label_count = max(label_count.values())\n",
    "\n",
    "# Calculate the difference needed for each class to reach the maximum count\n",
    "label_count_diff = {label: max_label_count - count for label, count in label_count.items()}\n",
    "\n",
    "label_count_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find how many augmentations should we do per sentence, by dividing the count diff by the number of sentences already in that class\n",
    "augmentation_count_needed = {label: diff // count for label, diff, count in zip(label_count.keys(), label_count_diff.values(), label_count.values())}\n",
    "augmentation_count_needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAPPING = {\n",
    "    0: \"Evidence\",\n",
    "    1: \"Claim\",\n",
    "    2: \"Concluding Statement\",\n",
    "    3: \"Lead\",\n",
    "    4: \"Position\",\n",
    "    5: \"Counterclaim\",\n",
    "    6: \"Rebuttal\"\n",
    "}\n",
    "\n",
    "developer_prompt = \"You are an expert in analyzing persuasive essays and understanding argumentative and discourse elements. The discourse elements are: Lead, Position, Claim, Counterclaim, Rebuttal, Evidence, and Concluding Statement. Lead refers to an introduction that begins with a statistic, a quotation, a description, or some other device to grab the reader's attention and point toward the thesis. Position refers to an opinion or conclusion on the main question. Claim refers to a claim that supports the position. Counterclaim refers to a claim that refutes another claim or gives an opposing reason to the position. Rebuttal refers to a claim that refutes a counterclaim. Evidence refers to ideas or examples that support claims, counterclaims, rebuttals, or the position. Concluding statement refers to a concluding statement that restates the position and claims.\"\n",
    "user_prompts = [] # {id: XX, prompt: YY}\n",
    "\n",
    "# random seed\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    for i in range(augmentation_count_needed[row[\"label\"]]):\n",
    "        prompt = \"This sentence is from the \" + MAPPING[row[\"label\"]] + \" discourse element:\\n\" + row[\"text\"] + \"\\n\\nFirst, think step by step on why this is a sentence of the \" + MAPPING[row[\"label\"]] + \" discourse element. Then, think step by step, and finally in the last line of your response (after putting a line break), please write a sentence that addresses the same topic, focusing on the \" + MAPPING[row[\"label\"]] + \" discourse element. You should use different names, words, and terminologies in your output, but the overall meaning and content should be the same and refer to the same discourse element. In the *last line* of your output, just put the sentence and nothing else.\"\n",
    "        user_prompts.append({\"id\": id, \"prompt\": prompt})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(user_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(user_prompts[0][\"prompt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "with open(\"../../../api_key.txt\", \"r\") as f:\n",
    "    api_key = f.read().strip()\n",
    "\n",
    "openai.api_key = api_key\n",
    "openai_client = OpenAI(api_key=api_key)\n",
    "\n",
    "def return_message_from_openai(messages, temperature = 1):\n",
    "    global openai_client\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=\"gpt-4o-2024-08-06\",\n",
    "        messages=messages,\n",
    "        temperature=temperature\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "jsonl_data = []\n",
    "\n",
    "for i, prompt in enumerate(user_prompts):\n",
    "    jsonl_data.append({\n",
    "        \"custom_id\": f\"request-{i+1}-id-{prompt['id']}\",\n",
    "        \"method\": \"POST\",\n",
    "        \"url\": \"/v1/chat/completions\",\n",
    "        \"body\": {\n",
    "            \"model\": \"gpt-4o-2024-08-06\",\n",
    "            \"messages\": [\n",
    "                {\"role\": \"developer\", \"content\": developer_prompt},\n",
    "                {\"role\": \"user\", \"content\": prompt[\"prompt\"]}\n",
    "            ],\n",
    "            \"temperature\": 1.0\n",
    "        }\n",
    "    })\n",
    "\n",
    "with open(f\"fifth-round-fold-{fold}.jsonl\", \"w\") as f:\n",
    "    for entry in jsonl_data:\n",
    "        f.write(json.dumps(entry) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_input_file = openai_client.files.create(\n",
    "    file=open(f\"fifth-round-fold-{fold}.jsonl\", \"rb\"),\n",
    "    purpose=\"batch\"\n",
    ")\n",
    "\n",
    "print(batch_input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_input_file_id = batch_input_file.id\n",
    "created_batch = openai_client.batches.create(\n",
    "    input_file_id=batch_input_file_id,\n",
    "    endpoint=\"/v1/chat/completions\",\n",
    "    completion_window=\"24h\",\n",
    "    metadata={\n",
    "        \"description\": f\"Fifth Round (fold {fold})\"\n",
    "    }\n",
    ")\n",
    "created_batch.id, created_batch.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = openai_client.batches.retrieve(created_batch.id)\n",
    "batch.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_response = openai_client.files.content(batch.output_file_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels_to_use = []\n",
    "keys = list(augmentation_count_needed.keys())\n",
    "for index, row in df.iterrows():\n",
    "    for i in range(augmentation_count_needed[row[\"label\"]]):\n",
    "        all_labels_to_use.append(row[\"label\"])\n",
    "\n",
    "len(all_labels_to_use), len([line for line in file_response.text.split(\"\\n\") if line])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a new dataframe id,text-id,text,label,original_id,test_fold, by using the df we had in the beginning\n",
    "result_df = pd.DataFrame(columns=[\"id\", \"text\", \"label\", \"original_id\"])\n",
    "\n",
    "i = 0\n",
    "for line in file_response.text.split(\"\\n\"):\n",
    "    if line:\n",
    "        data = json.loads(line)\n",
    "        # get the last line of  data[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"]\n",
    "        text = data[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"].split(\"\\n\")[-1]\n",
    "        new_row = {\n",
    "            \"id\": data[\"custom_id\"],\n",
    "            # \"text-id\": df[df[\"id\"] == int(data[\"custom_id\"].split(\"-\")[-1])][\"text-id\"].values[0],\n",
    "            \"text\": text,\n",
    "            \"label\": all_labels_to_use[i],\n",
    "            \"original_id\": data[\"custom_id\"]\n",
    "            # \"test_fold\": df[df[\"id\"] == int(data[\"custom_id\"].split(\"-\")[-1])][\"test_fold\"].values[0]\n",
    "        }\n",
    "        result_df = pd.concat([result_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "        i += 1\n",
    "\n",
    "result_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv(f\"fifth-round-fold-{fold}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mdev2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
